{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r9pNwz7OWYh-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab7407f-f839-498e-f18f-1f4ce4803378"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After this stage, if you are working on google colab you can switch to the T4 GPU to run heavy LLM models. However use cautiously as they can exhaust very quickly"
      ],
      "metadata": {
        "id": "QHFKRUUraPW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "# Use to see whether gpu connected or not"
      ],
      "metadata": {
        "id": "k5fQ3azdYt5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "#-------------------------------------------#\n",
        "#                 NLP TASK                  #\n",
        "#-------------------------------------------#\n",
        "'''\n",
        "1. Text classification: Assigning a category to a piece of text\n",
        "--> Sentiment Analysis\n",
        "--> Topic classification\n",
        "--> Spam detection\n",
        "'''\n",
        "classifier = pipeline(\"text-classification\")\n",
        "# All these are wrappers on top of hugging face platform: ready-made shortcut that wraps complex code into a simple and easy-to-use tool — so you don’t need to write everything from scratch.\n",
        "# Wrapper is somewhat similar to an API just that wrappers run locally within your code environment whereas APIs connect to external services\n",
        "\n",
        "'''\n",
        "2. Token classification: Assigning labels to individual tokens in a sequence.\n",
        "--> Named entity recognition (NER)\n",
        "--> Part-of-speech tagging\n",
        "'''\n",
        "token_classifier = pipeline(\"token-classification\")\n",
        "\n",
        "'''\n",
        "3. Question answering: Providing an answer to a question based on a given context.\n",
        "'''\n",
        "question_answerer = pipeline(\"question-answering\")\n",
        "\n",
        "'''\n",
        "4. Text generation: Generating new text based on a given prompt.\n",
        "--> Language modeling\n",
        "--> Story generation\n",
        "'''\n",
        "text_generator = pipeline(\"text-generation\")\n",
        "\n",
        "'''\n",
        "5. Summerization: Condensing long documents into shorter summaries\n",
        "'''\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "'''\n",
        "6. Translation: Translating text from one language to another\n",
        "'''\n",
        "translator = pipeline(\"translation\", model = \"Helsinki-NLP/opus-mt-en-fr\") # Copy the model id for model\n",
        "\n",
        "'''\n",
        "7. Text2Text Generation: General-purpose text transformation, including summerization, translation, and question answering.\n",
        "'''\n",
        "text2text_generator = pipeline(\"text2text-generation\")\n",
        "\n",
        "'''\n",
        "8. Fill-Mask: Predicting the masked token in a sequence\n",
        "'''\n",
        "fill_mask = pipeline(\"fill-mask\")\n",
        "\n",
        "'''\n",
        "9. Feature extraction: Transforming text into numerical representations / extracting hidden states or features from text\n",
        "'''\n",
        "feature_extractor = pipeline(\"feature-extraction\")\n",
        "\n",
        "'''\n",
        "10. Sentence Similarity: Measuring the similarity between 2 sentences\n",
        "'''\n",
        "sentence_similarity = pipeline(\"sentence-similarity\")"
      ],
      "metadata": {
        "id": "moWqFlOleqO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------#\n",
        "#          Computer Vision TASK             #\n",
        "#-------------------------------------------#\n",
        "\n",
        "'''\n",
        "1. Image Classification: Classyfying the main content of an image\n",
        "'''\n",
        "image_classifier = pipeline(\"image-classification\")\n",
        "\n",
        "'''\n",
        "2. Object Detection: Identifying objects within an image and their bounding boxes.\n",
        "'''\n",
        "object_detector = pipeline(\"object-detection\")\n",
        "\n",
        "'''\n",
        "3. Image Segmentation: Assigning a label to each pixel in an image / Segmenting different parts of an image into classes\n",
        "'''\n",
        "image_segmenter = pipeline(\"image-segmentation\")\n",
        "\n",
        "'''\n",
        "4. Image Generation: Creating new images based on a given prompt. / generating images from textual descriptions (like DALL-E)\n",
        "'''\n",
        "image_generator = pipeline(\"image-generation\")"
      ],
      "metadata": {
        "id": "1Oi41ptXjbWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------#\n",
        "#         Speech processing TASK            #\n",
        "#-------------------------------------------#\n",
        "'''\n",
        "1. Automatic Speech Recognition (ASR): Converting spoken language into written text\n",
        "'''\n",
        "asr = pipeline(\"automatic-speech-recognition\")\n",
        "\n",
        "'''\n",
        "2. Speech-to-Speech Translation: Converting spoken language into another spoken language\n",
        "'''\n",
        "speech_translator = pipeline(\"speech-to-speech-translation\")\n",
        "\n",
        "'''\n",
        "3. Audio Classification: Classifying the main content of an audio file (audio signals) into predefined categories\n",
        "'''\n",
        "audio_classifier = pipeline(\"audio-classification\")\n",
        "\n",
        "'''\n",
        "4. Audio Transcription: Converting spoken language into written text\n",
        "'''\n",
        "audio_transcription = pipeline(\"audio-transcription\")\n",
        "\n",
        "'''\n",
        "5. Audio-to-Audio Translation: Converting spoken language into another spoken language\n",
        "'''\n",
        "audio_translator = pipeline(\"audio-to-audio-translation\")\n",
        "\n",
        "'''\n",
        "6. Text-to-Speech (TTS): Converting text into spoken language\n",
        "'''\n",
        "tts = pipeline(\"text-to-speech\")\n",
        "\n",
        "'''\n",
        "7. Speech-to-Text (STT): Converting spoken language into written text\n",
        "'''\n",
        "stt = pipeline(\"speech-to-text\")\n",
        "'''\n",
        "8. Text-to-Speech (TTS): Converting text into spoken language\n",
        "'''\n",
        "tts = pipeline(\"text-to-speech\")\n"
      ],
      "metadata": {
        "id": "yxti8JdKkPvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------#\n",
        "#             Multimodal TASK               #\n",
        "#-------------------------------------------#\n",
        "\n",
        "'''\n",
        "1. Image captioning: Generating a textual description of an image\n",
        "'''\n",
        "image_captioner = pipeline(\"image-captioning\")\n",
        "\n",
        "'''\n",
        "2. Visual Question Answering(VQA): Answering questions about the content of an image\n",
        "'''\n",
        "visual_question_answerer = pipeline(\"visual-question-answering\")"
      ],
      "metadata": {
        "id": "v9z1wF2bloTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------#\n",
        "#                Other TASK                 #\n",
        "#-------------------------------------------#\n",
        "\n",
        "'''\n",
        "1. Table Question Answering: Answering questions based on tabular data\n",
        "'''\n",
        "table_qa = pipeline(\"table-question-answering\")\n",
        "\n",
        "'''\n",
        "2. Document question answering: Extracting answers from documents like pdfs\n",
        "'''\n",
        "doc_qa = pipeline(\"document-question-answering\") # Similar to RAG\n",
        "\n",
        "'''\n",
        "3. Time Series Forecasting: Predicting future values in the time series data (not directly supported in the main Transformer library)\n",
        "'''\n"
      ],
      "metadata": {
        "id": "tsXWusWImXp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Tasks"
      ],
      "metadata": {
        "id": "V8tSa7E_1Fwh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Sentiment analysis"
      ],
      "metadata": {
        "id": "kwWslXaR1IaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# If model not mentioned then it uses default model\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "result = classifier(\"I was so not happy with the second Mission Impossible Movie.\")"
      ],
      "metadata": {
        "id": "8x-x71g31HrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline(task = \"sentiment-analysis\")(\"I was confused with the Barbie Movie.\")"
      ],
      "metadata": {
        "id": "uzmYLbyR1_Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline(task = \"sentiment-analysis\")\\\n",
        "                                      (\"Everyday lots of LLM papers are published about LLMs Evaluation. \\\n",
        "                                      Lots of them look very promising. \\\n",
        "                                      I am not sure if we can actually evaluate LLMs. \\\n",
        "                                      there is still lots to do. \\\n",
        "                                      Don't you think?\")"
      ],
      "metadata": {
        "id": "1rsNmNg74f_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline(task = \"sentiment-analysis\", model = 'facebook/bart-large-mnli')\\\n",
        "                                      (\"Everyday lots of LLM papers are published about LLMs Evaluation. \\\n",
        "                                      Lots of them look very promising. \\\n",
        "                                      I am not sure if we can actually evaluate LLMs. \\\n",
        "                                      there is still lots to do. \\\n",
        "                                      Don't you think?\")"
      ],
      "metadata": {
        "id": "TwMuPxgA5In_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above used model is not really an LLM but it is more of a Language Model since it can only perforom the task of text classification.\n",
        "\n",
        "\n",
        "We will also try our hands on some LLMs like\n",
        "## **Llama, mistral, falcon, gemma**\n",
        "\n",
        "\n",
        "LM uses only transformer architecture. LLMs may use more than just this."
      ],
      "metadata": {
        "id": "lCGAQ6aV5U5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch sentiment analysis\n",
        "### Perform sentiment analysis on a bunch of lines in one go"
      ],
      "metadata": {
        "id": "_3ODzpTw67gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(task = \"sentiment-analysis\")\n",
        "\n",
        "task_list = [\"I really like Autoencoders, best models for Anomaly Detection\",\\\n",
        "             \"I am not sure if we Can actually Evaluate LLMs\",\\\n",
        "             \"PassiveAgressive is the name of a Linear Regression Model that so many people do not know\",\\\n",
        "             \"I hate long meetings.\"]\n",
        "# The difference between passing a batch of sentences and passing multiline corpus is that here it is like passing a list\n",
        "# We use \\ after the ending \" and we also use a ,\n",
        "classifier(task _list)"
      ],
      "metadata": {
        "id": "fAxGNt9U5eIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model only captures positive and negative emotions. If we want different different emotions then we can use another model."
      ],
      "metadata": {
        "id": "x5GDqIuX8Cid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(task = \"sentiment-analysis\", model = \"SamLowe/roberta-base-go_emotions\")\n",
        "\n",
        "task_list = [\"I really like Autoencoders, best models for Anomaly Detection\",\\\n",
        "             \"I am not sure if we Can actually Evaluate LLMs\",\\\n",
        "             \"PassiveAgressive is the name of a Linear Regression Model that so many people do not know\",\\\n",
        "             \"I hate long meetings.\"]\n",
        "# The difference between passing a batch of sentences and passing multiline corpus is that here it is like passing a list\n",
        "# We use \\ after the ending \" and we also use a ,\n",
        "classifier(task _list)"
      ],
      "metadata": {
        "id": "gWOc2Fu-8LBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Text Generation"
      ],
      "metadata": {
        "id": "l78RCcMz8dwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "text_generator = pipeline(\"text-generation\", model = 'distilbert/distilgpt2')\n",
        "generated_text = text_generator(\"Today is a rainy day in London\", truncation = True, num_return_sequences = 2)\n",
        "print(\"Generated_text:\\n \", generated_text[0]['generated_text'])"
      ],
      "metadata": {
        "id": "sai2B6YN8gXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Question answering"
      ],
      "metadata": {
        "id": "4yzllWxBDA49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa_model = pipeline(\"question-answering\")\n",
        "question = \"What is my job?\"\n",
        "context = \"I am developing AI models with Python.\"\n",
        "qa_model(question = question, context = context)"
      ],
      "metadata": {
        "id": "7-IxVAd6DDti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before using the LLM models we need to learn some genAI frameworks like LlamaIndex, LangChain as well as vector database"
      ],
      "metadata": {
        "id": "QtTYUMGMESlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FOR LLMS"
      ],
      "metadata": {
        "id": "FGrfQ_QjLqnL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tonkenization\n",
        "### Using Transformer technique with attention mechanism"
      ],
      "metadata": {
        "id": "A0q88crEDhcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification"
      ],
      "metadata": {
        "id": "HEeWLrknMA-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For auto tokenizer\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "mytokenizer1 = DistilBertTokenizer.from_pretrained(model_name)\n",
        "mymodel1 = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
        "# Tokenizer removes HTML tags, stop words, etc and later converts it to vector representation as well\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", model = mymodel2, tokenizer = mytokenizer2)\n",
        "res = classifier(\"I was so not happy with the Barbie Movie\")\n",
        "print(res)"
      ],
      "metadata": {
        "id": "CFAzdzQRDj-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand how tokenizer works:"
      ],
      "metadata": {
        "id": "k8HbRTM8Mvu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load a pre-trained tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Example text\n",
        "text = \"I was so not happy with the Barbie Movie\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.tokenize(text)\n",
        "\n",
        "# Print the tokens\n",
        "print(tokens)\n",
        "print(\"Tokens: \",tokens )\n",
        "\n",
        "# Convert tokens to input IDs / vectors\n",
        "input_ids = tokenizer.conver_tokens_to_ids(tokens)\n",
        "\n",
        "# IF YOU NEED TO PASS IT TO A MODEL THEN USE THE ENCODE METHOD BELOW NOT THE ABOVE METHOD\n",
        "\n",
        "# Encode the text (tokenization + converting to input IDs)\n",
        "encoded_input = tokenizer(text)\n",
        "print(\"Encoded input: \", encoded_input)\n",
        "'''\n",
        "Here in output the start of sentence is represented by 101 and the end is represented by 102 and in between we have the vector\n",
        "This is followed by token_type_ids and then attention_mask\n",
        "attention_mask (BINARY RESPONSE 0/1): attention mechanism; binary weightf assigned to tell the model which tokens to focus on and which not\n",
        "'''\n",
        "# Decode the text\n",
        "decoded_output = tokenizer.decode(input_ids)\n",
        "print(\"Decoded output: \", decoded_output)"
      ],
      "metadata": {
        "id": "giF2c7TFMyB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Attention mask hugging face glossary](https://huggingface.co/docs/transformers/glossary#attention-mask)"
      ],
      "metadata": {
        "id": "sPvqnWwAZbBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "token_type_ids: These IDs are used to distinguish between different sequences in tasks that involve multiple sentences, such as question-answering and sentence-pair classification. BERT uses this mechanism to understand which tokens belong to which segment. For single-sequence tasks like sentiment analysis, token_type_ids are all zeros\n",
        "\n",
        "attention_mask: Used to differentiate between actual tokens (if any). It helps focus on non-padding tokens and ignore padding tokens. A value of 1 indicates that the token should be attended to, while 0 indicates padding.\n",
        "\n",
        "Why padding tokens are used\n",
        "Uniform sequence lenght: depp learning models typically process input data in batches. To efficiently process these batches, all sequences in parallel without needing to handle variable length sequences individually"
      ],
      "metadata": {
        "id": "BYwgzAHUaB-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FINE TUNNING USING IMDB DATASET\n",
        "(Using LM model to perform sentiment analysis)"
      ],
      "metadata": {
        "id": "r8QZeydvbwJ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Install necessary libraries"
      ],
      "metadata": {
        "id": "HCIq2Ey6b0OR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets # See hugging face website for dataset\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "5Bejz1pPc6Yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Load and prepare the dataset"
      ],
      "metadata": {
        "id": "u5TQDv6ofE7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('imdb')\n",
        "\n",
        "# The model being used for the tokenizer should be the same as model used for training/inference"
      ],
      "metadata": {
        "id": "Pc8gvfUKc7EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "F985hAuiweXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "T3sfeOTCxJlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Preprocess the Data\n",
        "Tokenize the dataset using tokenizer associated with the pre-trained model"
      ],
      "metadata": {
        "id": "cJY6I85ZwjPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer\n",
        "tokeinzer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples['text'], padding = \"max_length\", truncation = True) # Passing as a dictionary\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched = True) # Apply on top of the entire dataset"
      ],
      "metadata": {
        "id": "NERd2nmiwoP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset\n",
        "tokenized_dataset[\"train\"]"
      ],
      "metadata": {
        "id": "SG3EboK4yflt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Set up the training arguments\n",
        "Specify the hyperparameters and training settings\n"
      ],
      "metadata": {
        "id": "teBHBR0ny8NO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"./results\",           # Output directory\n",
        "    eval_strategy = \"epoch\",            # Evaluate every epoch\n",
        "    learning_rate = 2e-5,               # Learning rate\n",
        "    per_device_train_batch_size = 16,   # Batch size for training\n",
        "    per_device_eval_batch_size = 16,    # Batch size for evaluation\n",
        "    num_train_epochs = 1,               # Number of training epochs\n",
        "    weight_decay = 0.01)                # Strength of weight decay\n",
        "training_args"
      ],
      "metadata": {
        "id": "dXpNK6_5yyXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Initialize the model\n",
        "Load the pre-trained model and define the training procedure"
      ],
      "metadata": {
        "id": "UI3_HCPs0g8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, Trainer\n",
        "# Load the pre-trained model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 2)\n",
        "\n",
        "# Initialize the trainer\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = tokenized_datasets[\"train\"],\n",
        "    eval_dataset = tokenized_datasets[\"test\"])\n",
        "# Training will not start here as we need to call the model to start training"
      ],
      "metadata": {
        "id": "VuEVKKa-0aCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Train the model\n",
        "Fine tune the pre-trained model on your specific dataset"
      ],
      "metadata": {
        "id": "R5KTDjAz1Ulj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "qtheNZBJ0K0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Evaluate the model\n",
        "Assess the model's performance on validation set"
      ],
      "metadata": {
        "id": "oD99_xt115Os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainer.evaluate()\n",
        "print(results)"
      ],
      "metadata": {
        "id": "pnTOMiNa1-_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Save the fine-tuned model\n",
        "Save the fine tuned model for later use"
      ],
      "metadata": {
        "id": "0cTmI8H32EmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('./fine-tuned-model')\n",
        "tokenizer.save_pretrained('./fine-tuned-model')\n",
        "# For new data use this pretrained model and tokenizer"
      ],
      "metadata": {
        "id": "oUUOk26j2Xgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ArXiv Project\n",
        "ArXiv is a research article website"
      ],
      "metadata": {
        "id": "lbkTa4zQ3n_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv"
      ],
      "metadata": {
        "id": "7VhKmSAG3vR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "IKJe-RhS20nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query to fetch AI-related papers\n",
        "query = 'ai OR artificial intelligence OR machine learning'\n",
        "search = arxiv.Search(query = query, max_results = 10, sort_by = arxiv.SortCriterion.SubmittedDate)\n",
        "\n",
        "# Fetch papers\n",
        "papers = []\n",
        "for result in search.results():\n",
        "  papers.append({\n",
        "      'published': result.published,\n",
        "      'title': result.title,\n",
        "      'abstract': result.summary,\n",
        "      'categories': result.categories\n",
        "  })\n",
        "df = pd.DataFrame(papers)\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "fHWEYDid3-Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to perform summarization of research paper perform the following steps:"
      ],
      "metadata": {
        "id": "k7B07BMb5HGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example abstract from API\n",
        "abstract = df['abstract'][0]\n",
        "summarizer = pipeline(\"summarization\", model = 'facebook/bart-large-cnn')\n",
        "summarization_result = summarizer(abstract)"
      ],
      "metadata": {
        "id": "g_lTMBP05MGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_result[0]['summary_text']"
      ],
      "metadata": {
        "id": "DqeX75Lm5fiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes we need hugging face API key in order to use models\n",
        "\n",
        "## DO NOT SHARE YOUR ACCESS TOKEN WITH ANYONE. ESPECIALLY DO NOT PUT IT ON PUBLIC WEBSITES LIKE GITHUB."
      ],
      "metadata": {
        "id": "jcScwCWo6Asn"
      }
    }
  ]
}