{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## STEPS TO FOLLOW:\n",
        "\n",
        "1. Import the requirements\n",
        "2. Change to GPU\n",
        "3. Get the data\n",
        "4. Apply / Map the tokenizer\n",
        "5. Set the training arguments\n",
        "6. Train the model\n",
        "7. Evaluation using required metrics\n",
        "8. Saving the model to use on test data\n",
        "9. Prediction"
      ],
      "metadata": {
        "id": "WNONSHuYMCDK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8ndJpexAAH9"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q"
      ],
      "metadata": {
        "id": "8Z0MKRKdBI3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade accelerate # Helps to use GPU / CUDA\n",
        "!pip uninstall -y transformers accelerate\n",
        "!pip install transformers accelerate\n",
        "# In order to use the updated version"
      ],
      "metadata": {
        "id": "5JKbhqnSFYTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "from datasets import load_dataset, load_from_disk, load_metric\n",
        "import matplotlib.plotly as plt\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch # Importing pytorch\n",
        "\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "T4wVsefAFlS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To check if cuda is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "eqsYqi5MGOed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
        "tokenizer = AutomTokenizer.from_pretrained(model_ckpt)\n",
        "# To download the tokenizer"
      ],
      "metadata": {
        "id": "DpVBxyk0GT6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n",
        "# To download the model"
      ],
      "metadata": {
        "id": "8hsrm7cpHIeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detaset_samsum = load_dataset(\"samsum\")"
      ],
      "metadata": {
        "id": "sv8-IwPyHm_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_samsum"
      ],
      "metadata": {
        "id": "mH1PUxMzH7kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_samsum[\"train\"][\"dialogue\"][1]"
      ],
      "metadata": {
        "id": "xULGUHRmH8vE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_samsum[\"train\"][1][\"summary\"]"
      ],
      "metadata": {
        "id": "fxkD5X4wKqZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prints number of examples available in this dataset\n",
        "split_lengths = [len(dataset_samsum[split]) for split in dataset_samsum]\n",
        "print(f\"Split lengths: {split_lengths}\")\n",
        "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
        "print(\"\\nDialogue: \\n\")\n",
        "print(dataset_samsum[\"test\"][1][\"dialogue\"])\n",
        "print(\"\\nSummary: \\n\")\n",
        "print(dataset_samsum[\"test\"][1][\"summary\"])"
      ],
      "metadata": {
        "id": "27TNCaVTKqTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess data to convert to vector representation\n",
        "def convert_examples_to_features(example_batch):\n",
        "    input_encodings = tokenizer(example_batch[\"dialogue\"], max_length=1024, truncation=True)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        target_encodings = tokenizer(example_batch[\"summary\"], max_length=128, truncation=True)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_encodings[\"input_ids\"],\n",
        "        \"attention_mask\": input_encodings[\"attention_mask\"],\n",
        "        \"labels\": target_encodings[\"input_ids\"]\n",
        "    }"
      ],
      "metadata": {
        "id": "bVUKhKfCKqJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched=True)"
      ],
      "metadata": {
        "id": "QTGsn45HLPsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_samsum_pt[\"train\"]"
      ],
      "metadata": {
        "id": "SdxSOzbULPpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_samsum_pt[\"train\"][\"input_ids\"][1]"
      ],
      "metadata": {
        "id": "A-skq3TmLPmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_samsum_pt[\"train\"][\"attention_mask\"][1]"
      ],
      "metadata": {
        "id": "m3neTZDvLPc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_samsum_pt[\"train\"][\"labels\"][1]"
      ],
      "metadata": {
        "id": "-ojXLRQeMV5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAINING"
      ],
      "metadata": {
        "id": "e8iVT0mwMSrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)\n",
        "# Data collator helps load data in batches in the memory as we only have fixed RAM"
      ],
      "metadata": {
        "id": "Rh49YWQDMSdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "trainer_args = TrainingArguments(\n",
        "    output_dir=\"pegasus-samsum\", num_train_epochs=1, warmup_steps=500,\n",
        "    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
        "    weight_decay=0.01, logging_steps=10,\n",
        "    evaluation_strategy=\"steps\", eval_steps=500, save_steps=1e6,\n",
        "    gradient_accumulation_steps=16)"
      ],
      "metadata": {
        "id": "-XU_bSqSMSYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model=model_pegasus, args=trainer_args,\n",
        "                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
        "                  train_dataset=dataset_samsum_pt[\"train\"],\n",
        "                  eval_dataset=dataset_samsum_pt[\"validation\"])\n",
        "\n",
        "# Instead of train data you can also use test data if you think that there is not enough time/memory space"
      ],
      "metadata": {
        "id": "RRe_xhyKMSV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "a40ODWEEMSNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EVALUATION\n",
        "The error here should be very close to 0"
      ],
      "metadata": {
        "id": "co0UPHmkPaKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
        "    \"\"\"split the dataset into smaller batches so that we can process simultaneously\n",
        "    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
        "    for i in range (0, len(list_of_elements), batch_size):\n",
        "      yield list_of_elements[i : i + batch_size]\n",
        "\n",
        "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n",
        "                                batch_size = 16, device = device,\n",
        "                                column_text = \"article\",\n",
        "                                column_summary = \"highlights\"):\n",
        "  article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
        "  target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
        "\n",
        "  for article_batch, target_batch in tqdm(zip(article_batches, target_batches), total = len(article_batches)):\n",
        "    inputs = tokenizer(article_batch, max_length = 1024, truncation = True,\n",
        "                       padding = \"max_length\", return_tensors = \"pt\")\n",
        "    summaries = model.generate(input_ids = inputs[\"input_ids\"].to(device),\n",
        "                               attention_mask = inputs[\"attention_mask\"].to(device),\n",
        "                               length_penalty = 0.8, num_beams = 8, max_length = 128)\n",
        "    '''\n",
        "    Parameter for length penalty ensures that the model does not generate sequences that are too short or too long. Close to 0 means short and close to 1 means long\n",
        "    Text summarization uses rogue score, just how classification uses other types of performance metrics\n",
        "    Now we decode the generated texts, replace the token and add the decoded texts with the references to the metric.\n",
        "    '''\n",
        "\n",
        "    decoded_summaries = [tokenizer.decode(s, skip_special_tokens = True,\n",
        "                                          clean_up_tokenization_spaces = True)\n",
        "                          for s in summaries]\n",
        "    metric.add_batch(predictions = decoded_summaries, references = target_batch)\n",
        "# Finally compute and return the ROUGE scores.\n",
        "  score = metric.compute()\n",
        "  return score"
      ],
      "metadata": {
        "id": "qAD7rwhRPNtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "rouge_metric = load_metric(\"rouge\")"
      ],
      "metadata": {
        "id": "rGaah6jjUIDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = calculate_metric_on_test_ds(\n",
        "    dataset_samsum[\"test\"][0:10], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = \"dialogue\", column_summary = \"summary\"\n",
        ")\n",
        "# Here we have just taken 10 as space is less but for actual model evaluation just omit the [0:10] part\n",
        "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
        "pd.DataFrame(rouge_dict, index = [f\"pegasus\"])"
      ],
      "metadata": {
        "id": "V8PFQB-SUL31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rouge score should be close to 1"
      ],
      "metadata": {
        "id": "6_9mHQmwVq3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAVING THE MODEL"
      ],
      "metadata": {
        "id": "bvC5UTePUvJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_pegasus.save_pretrained(\"pegasus-samsum-model\")"
      ],
      "metadata": {
        "id": "RRG3l_aKUL1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"tokenizer\")"
      ],
      "metadata": {
        "id": "eE-i2U26ULzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PREDICTION"
      ],
      "metadata": {
        "id": "cM01gohpWSc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/tokenizer\")"
      ],
      "metadata": {
        "id": "rV5BagXeULw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]"
      ],
      "metadata": {
        "id": "YjDttE9CXc3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference = dataset_samsum[\"test\"][0][\"summary\"]"
      ],
      "metadata": {
        "id": "Jn1iQxvLXdxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\": 8, \"max_length\": 128}\n",
        "sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n",
        "reference = dataset_samsum[\"test\"][0][\"summary\"]\n",
        "pipe = pipeline(\"summarization\", model = \"pegasus-samsum-model\", tokenizer = tokenizer)\n",
        "# Here we pass name of our model\n",
        "\n",
        "print(\"Dialogue: \")\n",
        "print(sample_text)\n",
        "\n",
        "print(\"\\nReference Summary: \")\n",
        "print(reference)\n",
        "\n",
        "print(\"\\nModel Summary: \")\n",
        "print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])"
      ],
      "metadata": {
        "id": "WrxPevrnWPCF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}