This repository reflects how to work with LLMs. 

We also understand the difference between LM and LLMs. 
Then we move on to understanding Hugging face platform to use open source models. We see the Transformer library and understand Tokenizer. After training models, we evaluate and save them.
What we learnt from LMs and hugging face:
## STEPS TO FOLLOW:

1. Import the requirements
2. Change to GPU
3. Get the data
4. Apply / Map the tokenizer
5. Set the training arguments
6. Train the model
7. Evaluation using required metrics
8. Saving the model to use on test data
9. Prediction
    
* First try implementing a pretrained model using a pipeline from hugging face task section.
* If that does not give good results then try fine tuning (adding more knowledge) on top of pretrained using Parameter Efficient Fine Tuning (PEFT)

Now we will be moving onto understanding LangChain along with vector databases.
